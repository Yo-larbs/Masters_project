---
title: "masters eo project"
output:
  html_document: default
  pdf_document: default
date: "2025-01-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

installing packages

```{r}
#install.packages("factoextra")
#install.packages("tidyr")
#install.packages("readr")
#install.packages("ggplot2")
#install.packages("DescTools")
#install.packages('ggrepel')
#install.packages("minpack.lm")
#install.packages("drc")
#install.packages("purrr")
library(tidyr)
library(readr)
library(dplyr)
library(ggplot2)
library(DescTools)
library(ggrepel)
library(minpack.lm)
library(drc)
library(factoextra)
library(purrr)
```

we now load in the data file downloaded from the drive

```{r}
setwd('~/Documents/Masters_project/')
HIV1_vector_data=read_csv('inputs/Cleaning_and_classification/HIV1_vectors_collated_n0_uncleaned(Sheet1).csv')
```

need to clean data, cleaning up NAs and empty rows in the original dataset, only keeping useful column we also get rid of NA

```{r}
HIV1_vector_data=HIV1_vector_data[,c(2,5,6,7:13,15)]
HIV1_vector_data=na.omit(HIV1_vector_data)

```

here we get rid of reduced values, this normally represents dying cells which is irrelevant to our analysis. in this code if a data point with a higher volume within a replicate is less than 80% of the max for that replicate it is replaced with the maximum. This practically means that as the graph increases it plateaus at the max instead of decreasing

```{r}
library(dplyr)
HIV1_vector_data=HIV1_vector_data%>%group_by(cell_line,screen_nb,replicate)%>% 
  mutate(max_gfp=max(assay_output),max_gfp_titre=which.max(assay_output)) %>%
mutate(assay_output=if_else((infection_volume_ul>infection_volume_ul[max_gfp_titre] & assay_output  <= 0.8 * max_gfp),max_gfp,assay_output))%>% dplyr::select(-max_gfp,-max_gfp_titre) %>% ungroup()
```

subsetting data for each cell line, creates a list of dataframes for each cell line (makes it easier to manipulate) per cell line

```{r}
vector_data_per_cell_line=HIV1_vector_data%>%nest_by(cell_line,.keep = T)
```

this points all plots by titre amount and separated by screen, allows you to see general shape of data, separated by

```{r}
apply_plot=function(i){ggplot(data = i,aes(x=infection_volume_ul,y=assay_output,group = interaction(screen,replicate) ))+
  geom_point()+
  ggtitle(i$cell_line[1])+
  theme_classic()
}
```

```{r}
scatter_plot_vector=purrr::map(vector_data_per_cell_line$data,apply_plot)
#names(scatter_plot_vector)=vector_data_per_cell_line$cell_line
scatter_plot_vector
```

making a boxplot of the assay output per cell and per titre, can use this to see distribution between screens and replicates and see the initial outliers, stat summary lets you see the upper quartile, median, lower quartile and max and min values

```{r}
apply_plot_boxplot=function(i){ggplot(data = i,aes(y=assay_output,x=titre,))+
  geom_boxplot()+
  ggtitle(i$cell_line[1])+
  stat_summary(geom="text", fun.y=quantile,
               aes(label=sprintf("%1.1f", ..y..), color=factor(titre)),
               position=position_nudge(x=0.6), size=3.5) +
  theme_classic()
}
```

code to apply function

```{r}
box_plot_vector=purrr::map(vector_data_per_cell_line$data,apply_plot_boxplot)
names(box_plot_vector)=vector_data_per_cell_line$cell_line
box_plot_vector
```

### dealing with outliers

Now i want to get rid of outliers, however i can't just get rid of all outlier values because they might actually be biologically relevant, have to check with other values in the replicates or screen,

creating outlier function which determines if the value is outlier by $(1.5 \times IQR + UQ)$ or $1.5 \times IQR - LQ$

```{r}
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}

```

this is a function that applies outlier per titre (checks if value is a outlier compared to other values in other screens)

```{r}
outlier_function=function(i){
  i %>%
  group_by(titre) %>%
  mutate(outlier = ifelse(is_outlier(assay_output), assay_output, as.numeric(NA))) 
}

```

use this to remove outlying observations. per screen (per cell) there are 5 titres (with 2 replicates ), if at least 3 of these points are outliers that set of points for that replicate is removed (whole observation may be outlier )

```{r}
all_outliers=purrr::map(vector_data_per_cell_line$data,outlier_function)
HIV1_vector_data=bind_rows(all_outliers)%>%group_by(cell_line,screen_nb,replicate)%>%filter(((sum(is.na(outlier)))>2))

```

## normalising between the screens

In order to get rid of any batch effects from the screening process, we want to find the zscore to normalise within the screen, this makes it more comparable between screens

plot the boxplot for each screen, before normalisation

```{r}
ggplot(data = HIV1_vector_data,aes(y=assay_output, x=as.factor(screen_nb),group = screen_nb,))+
  geom_boxplot()+
  ggtitle('change between screens')+
  theme_classic()
```

okay going to apply zscore to all values to standardise between screens to try and get rid of any batch effects to allow proper comparison

```{r}
#HIV_vector_group_data = HIV1_vector_data%>%summarise(mean_output=mean(assay_output),sd_output=sd(assay_output))
HIV1_vector_data=HIV1_vector_data%>%
  group_by(screen_nb)%>%
  mutate(zscore=((((assay_output-mean(assay_output))/sd(assay_output)))))%>%
  ungroup()
```

making box plot for after normalisation

```{r}
ggplot(data = HIV1_vector_data,aes(y=zscore, x=as.factor(screen_nb),group = screen_nb,))+
  geom_boxplot()+
  ggtitle('change between screens')+
  theme_classic()
```

## finding parameters

so now i have the normalised max mean percentage (in z score) A bit worried that by normalising it and getting rid of the difference it might affect results

finding now the area under the curve using AUC function

```{r}
HIV1_vector_data=HIV1_vector_data%>%group_by(batch,cell_line,screen,screen_nb,replicate)%>%
  mutate(area_under_curve=AUC(infection_volume_ul,(zscore)))%>%
  ungroup()
```

updating the df list per cell line

```{r}
vector_data_per_cell_line=HIV1_vector_data%>%nest_by(cell_line,.keep = T)
```

# fitting the curves

##logarithmic

Here we use a logarithmic $$a+b \times log_2(x-c)$$ with 3 parameters, a- y offset b- slope and c- x offset, to help plot the graph in our analysis a higher a and b should correspond to more permissive/susceptible and a lower c would correspond to less permissive. However this depends on how the data is fitted, the fit may mean these values aren't directly correlated with those phenotype

```{r}
logarithmic_func <- function(x, a, b,c) {
  a + b * log(x-c,base = 2)
}

```

this is the function for fitting. using a robust nlsLM function, fits the data using start points, having to choose good starting volumes, saves coefs to the database. There is also error handling using tryCatch- giving error message and instead adds NA

```{r,message=FALSE,include=FALSE}
logarithmic_func <- function(x, a, b) {
  a + b * log(x,base = 2)
}
fit_log <- function(data) {

  # Fit the logarithmic model for each group and add coefficients to the data frame
  fitted_data <- data %>%
    group_by(screen_nb,replicate) %>%
    group_modify(~ {
      tryCatch({
        # Fit the logarithmic model
        model <- nlsLM(
          zscore ~ logarithmic_func(infection_volume_ul, a, b),
          data = .x,
          start = list(a = 0, b = -1),# c = -0.0001),
          control = nls.control(maxiter = 1024)
        )
        
        # Extract coefficients
        coefs <- coef(model)
        
        # Add coefficients back to the group's data
        .x %>%
          mutate(a = coefs["a"], b = coefs["b"])#, c = coefs["c"])
      }, error = function(e) {
        # If fitting fails, add NA values for coefficients
        .x %>%
          mutate(a = NA, b = NA)#, c = NA)
      })
    }) %>%
    ungroup()  # Ungroup after processing
  
  return(fitted_data)
}
# Assuming your data frame is named 'df' and has columns 'screen', 'infection_volume_ul', and 'mean(zscore)'
vector_data_per_cell_line=purrr::map(vector_data_per_cell_line$data,fit_log)%>%bind_rows()%>%nest_by(cell_line,.keep = T)
```

Plotting function, using ggplot, use logarithmic function to create predicted list of values, these values are then fed into geom line to be the fitted line. then add the parameters onto the graph averaged for each cell line

```{r}
apply_plot_log=function(i){
i <- i %>%
    mutate(predicted = logarithmic_func(infection_volume_ul, i$a, i$b))#,i$c))
  
  # Create the plot
  p=ggplot(data = i, aes(x = infection_volume_ul, y = zscore, group = interaction(screen_nb,replicate))) +
    geom_point(aes(color=screen_nb),color = "blue", size = 2, alpha = 0.6) + # Actual data points
    geom_line(aes(y = predicted,color=screen_nb), color = "red", size = 1) + # Fitted line
    ggtitle(unique(i$cell_line)) + # Dynamic title
    labs(x = "Infection Volume (uL)", y = "Mean Z-Score") +
    theme_classic()
  
    params_text <- paste0(
    "Parameters:\n",
    "a = ", round(mean(i$a, na.rm = TRUE), 2), "\n",
    "b = ", round(mean(i$b, na.rm = TRUE), 2), "\n"
    #"d = ", round(mean(i$c, na.rm = TRUE), 2), "\n"
  )
  
  p + annotate("text",
               x = Inf, y = -Inf,
               label = params_text,
               hjust = 1.1, vjust = -0.1,
               size = 3)
}
```

```{r}
logarithmic_plot_vector=purrr::map(vector_data_per_cell_line$data,apply_plot_log)
names(logarithmic_plot_vector)=vector_data_per_cell_line$cell_line
logarithmic_plot_vector
```

## logistic

Decided to use to use a 4 parameter logistic, $$c+\frac{d-c}{1+e^{b(x-e)}}$$

c - min,d-max, b-slope, e- x offset a higher c and d and b should correspond to an increase in permissiveness, an increase in e should correspond with lower permissiveness , however the fitting of the graph may not fit completely accurately, e.g parameters such as b and c may be extra large/small to allow a more accurate fit compared to what the values actually represent. So said parameters may have to be reevaluated later in the analysis

I use Drc (dose response package ) as they provide a robust 4 parameter fitting with drm function, i then take the coefficients out of the model and store them, per replicate

```{r,include=FALSE}
# Correct 4PL formula (parentheses fix)
logistic_func <- function(x, b, d, e,c){
 c+ ((d-c)/(1 + exp(b*(x -e))))# Fixed denominator placement
}

# Robust fitting function
logistic_fit <- function(data) {
    fitted_data <- data %>%
        group_by(screen_nb,replicate) %>%  # Verify these columns exist in your data
        group_modify(~ {
            tryCatch({ # Fit with drm, explicitly stating NO log transformation
                model <- drm(zscore ~ infection_volume_ul, fct = L.4(),lowerl = c(-Inf, min(.x$zscore), -Inf, -Inf), data = .x,control = drmc(relTol = 1e-06,method = "CG"))
                coefs <- coef(model)

                .x %>%
                    mutate(
                        logis_b = coefs["b:(Intercept)"],
                        logis_e = coefs["e:(Intercept)"],
                        logis_c = coefs["c:(Intercept)"],
                        logis_d = coefs["d:(Intercept)"]
                    )
            }, error = function(e) {
                warning(paste("drc error for screen_nb:", .x$screen_nb[1], ":", e$message))
                .x %>%
                    mutate(logis_b = NA, logis_e = NA, logis_d = NA)
            })
        }) %>%
        ungroup()
    return(fitted_data)
}

# Apply to cell line data
vector_data_per_cell_line <- purrr::map(vector_data_per_cell_line$data,logistic_fit)%>%bind_rows()%>%nest_by(cell_line,.keep = T)


```

similar to plot logarithmic, i create predicted then use ggplot to plot the line over the actual points to assess the fit, and use the parameters to see if the values i get for each parameter make biological sense

```{r}
apply_plot_logistic <- function(i) {
  # Generate predictions using row-wise parameters
  plot_data <- i %>%
    mutate(
      predicted = logistic_func(
        x = infection_volume_ul,
        b = logis_b,
        c = logis_c,
        d = logis_d,
        e = logis_e
      )
    )
  
  # Create base plot
  p <- ggplot(plot_data, aes(x = infection_volume_ul, y = zscore)) +
    # Actual data points
    geom_point(
      aes(color = factor(screen_nb)),  # Color by screen_nb
      size = 2,
      alpha = 0.6,
      show.legend = T
    ) +
    # Fitted curves
    geom_line(
      aes(y = predicted, group = interaction(screen_nb, replicate)),
      color = "red",
      linewidth = 1
    ) +
    # Labels and theme
    labs(
      title = unique(plot_data$cell_line),
      x = "Infection Volume (microL)",
      y = "Mean Z-Score"
    ) +
    theme_classic(base_size = 12)
  
  # Add regression parameters annotation
  params_text <- paste0(
    "Parameters:\n",
    "b = ", round(mean(plot_data$logis_b, na.rm = TRUE), 2), "\n",
    "c = ", round(mean(plot_data$logis_c, na.rm = TRUE), 2), "\n",
    "d = ", round(mean(plot_data$logis_d, na.rm = TRUE), 2), "\n",
    "e = ", round(mean(plot_data$logis_e, na.rm = TRUE), 2)
  )
  
  p + annotate("text",
               x = Inf, y = -Inf,
               label = params_text,
               hjust = 1.1, vjust = -0.1,
               size = 3)
}
```

```{r}
logistic_plot_vector=purrr::map(vector_data_per_cell_line$data,apply_plot_logistic)
names(logistic_plot_vector)=vector_data_per_cell_line$cell_line
logistic_plot_vector
```

# averaging and cleaning data

I now unnest from the cell line dataframe list to get a dataframe containing all my parameters, i then group it and summarise it to get the mean for all the parameters across replicates

```{r}
HIV1_vector_data=vector_data_per_cell_line%>%unnest()

HIV1_vector_data_mean=HIV1_vector_data%>%
  group_by(batch,cell_line,screen,screen_nb,titre,infection_volume_ul)%>%
  summarise(assay_output=mean(assay_output),
            zscore=mean(zscore),
            a_log=mean(a),
            b_log=mean(b),
            #c_log=mean(c),
            logis_b=mean(logis_b),
            logis_d=mean(logis_d),
            logis_c=mean(logis_c),
            logis_e=mean(logis_e),
            area_under_curve=mean(area_under_curve))%>%
  ungroup()

HIV1_vector_data_mean <- HIV1_vector_data_mean %>%
  mutate(across(c(zscore, a_log, b_log,#c_log, 
                  logis_b, logis_d,logis_c,logis_e, area_under_curve), as.numeric))

```

i then create a dataframe containing the parameter information for each cell screen, so one row per screen and cell line

```{r}
HIV1_vector_data_PCA=unique(HIV1_vector_data_mean%>%
                              group_by(cell_line,screen_nb)%>%
                              summarise(assay_output=max(zscore),
                                        a_log=a_log,
                                        b_log=(b_log),
                                        #c_log=c_log,
                                        logis_b=logis_b,
                                        logis_d=logis_d,
                                        logis_c=logis_c,
                                        logis_e=logis_e,
                                        area_under_curve=area_under_curve))
```

Then averaging between screens,so i get a dataframe with each cell line by itself with its parameters

```{r}

HIV1_vector_data_PCA_sum <- HIV1_vector_data_PCA %>%
  group_by(cell_line) %>%
  summarise(
    assay_output      = mean(assay_output, na.rm = TRUE),
    a_log             = mean(a_log, na.rm = TRUE),
    b_log             = mean(b_log, na.rm = TRUE),
    #c_log            = mean(c_log, na.rm = TRUE),
    logis_b           = -mean(logis_b, na.rm = TRUE),
    logis_d           = mean(logis_d, na.rm = TRUE),
    #logis_c           = mean(logis_c, na.rm = TRUE),
    #logis_e           = mean(logis_e, na.rm = TRUE),
    area_under_curve  = abs(mean(area_under_curve, na.rm = TRUE))
  )
```

i Then rescale it for the PCA, allowing the different parameters to be included(by scaling it one parameter with a different range doesn't have a skewing effect on the data)

i then turn it into a matrix to set the row names as the cell names,

```{r}
HIV1_vector_data_PCA_1=HIV1_vector_data_PCA_sum%>%mutate_if(is.numeric,scale)


HIV1_vector_data_PCA_1=subset(HIV1_vector_data_PCA_1, select=-cell_line)

HIV1_vector_data_PCA_1=as.matrix(HIV1_vector_data_PCA_1)

row.names(HIV1_vector_data_PCA_1)=HIV1_vector_data_PCA_sum$cell_line



```

here i am checking the correlation but there is a visualization for this later

```{r}
HIV1_vector_data_PCA_1=as.data.frame(HIV1_vector_data_PCA_1)
#cor(x=HIV1_vector_data_PCA_1$logis_c,y=HIV1_vector_data_PCA_1$a_log)
```

```{r}
wssplot <- function(data, nc=15, seed=1234){
                  wss <- (nrow(data)-1)*sum(apply(data,2,var))
                      for (i in 2:nc){
                set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
              plot(1:nc, wss, type="b", xlab="Number of Clusters",
                            ylab="Within groups sum of squares")
              wss
}
wssplot(HIV1_vector_data_PCA_1)
```

#PCA

i used prcomp for my pca analysis, also used factoextra which gives me some more tools to help visualise the data, it generates an object that allows me to visualise data

```{r}
PCA=prcomp(x = HIV1_vector_data_PCA_1)
```

I generate a graph to show me my contributions for each principle component, normally majority of difference in variance is explained by PC1 and PC2

```{r}
library(factoextra)
fviz_eig(PCA)
```

This is a plot of the points on my graph by PC1 and PC2 , can visualise the spread

```{r}
fviz_pca_ind(PCA,col.ind = "cos2",repel = F)

```

This shows how each of my parameters contribute to the variance explained in PC1 and PC2, the more direction a parameter points to the more that parameter contributes to the. variance explained by that Principle component. Contributions are shown by colour, with blue meaning it has little contribution to the variance in the PCs

```{r}
fviz_pca_var(PCA,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE ) 
```

Overlays data points with the parameters to see how each datapiont is in parameter and can see which parameters are causing the split

```{r}
fviz_pca_biplot(PCA, repel=F,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```

In this snippet i get rid of positive and negative controls for dataset as it would affect the quantiles i use to determine extremes and would take up other cell lines in that quantile e.g the top 10%.

I then plot a graph showing how data is organised only on the PC with the highest percentage of variance (PC1), this gives me a linear graph of points. I added jitter to make the points more readable but the y axis in the graph doesn't represent anything at all

```{r}

res.ind <- get_pca_ind(PCA)
dim(res.ind$coord)
ind_coord <- as.data.frame(res.ind$coord)
ind_coord=ind_coord[-which(row.names(ind_coord)==('293T')|row.names(ind_coord)=='CTR_M2_O5'|row.names(ind_coord)=='CTR_M2_O5_21'),]
res.ind$cos2=res.ind$cos2[-which(row.names(res.ind$cos2)==('293T')|row.names(res.ind$cos2)=='CTR_M2_O5'|row.names(res.ind$cos2)=='CTR_M2_O5_21'),]

ggplot(data = ind_coord,aes(x=Dim.1,y = 0,label=row.names(ind_coord)))+
  geom_point(size = 3,aes(colour = res.ind$cos2[,1]), position = position_jitter(height = 0.02,width = 0),)+
  geom_label_repel(aes(label=ifelse(Dim.1 < quantile(Dim.1, 0.1) | Dim.1 > quantile(Dim.1, 0.9),row.names(ind_coord),'')),max.overlaps = 150)+
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray")+
  theme_minimal() +
  coord_cartesian(ylim = c(-0.05, 0.05))+
  theme(axis.ticks.y = element_blank(),
        axis.text.y  = element_blank(),
        panel.grid.major.y = element_blank())
         
```

#classification

Here i work on my classification, because the PC1 is only about 50% means that i can't trust it properly classify my data, but just in case i keep the upper and lower 10% of the points for dimension 1

but i also create a weighted score, combining both PC1 and PC2 to properly split up my data. I try to use it by taking the percentages of PC1 and PC2 and rescaling it to 100% and then multiplying the PC by its scaled percentage. When you look at the directions of parameters and the spread of the cell lines, assay output and area under curve seem to account for most of the variation, so by weighting the PC, you can classify more accurately the top and bottom quantiles.

I also plot the points with k means clustering alogirthm, and i label based on the top quantiles for my combined score, i save the top and bottom 10% to their own dataframes for storage

```{r}
susceptible_vector_cell=ind_coord[(ind_coord$Dim.1 < quantile(ind_coord$Dim.1, 0.1)),]
Resistant_vector_cell=ind_coord[(ind_coord$Dim.1 > quantile(ind_coord$Dim.1, 0.9)),]

HIV_vector_data_susceptible=HIV1_vector_data_PCA_1[match(row.names(susceptible_vector_cell),(HIV1_vector_data_PCA_sum$cell_line)),]
HIV_vector_data_resistant=HIV1_vector_data_PCA_1[match(row.names(Resistant_vector_cell),(HIV1_vector_data_PCA_sum$cell_line)),]

set.seed(18)
clusters <- kmeans(ind_coord[, c("Dim.1", "Dim.2")], centers = 8)
ind_coord$group <- as.factor(clusters$cluster)
ind_coord$pc_combined=0.80*ind_coord$Dim.1+ 0.20*ind_coord$Dim.2

ggplot(data = ind_coord,aes(x=Dim.1,y = Dim.2,label=row.names(ind_coord)))+
  geom_point(size = 3,aes(colour = group))+
  geom_label_repel(aes(label=ifelse(pc_combined < quantile(pc_combined, 0.1) | pc_combined > quantile(pc_combined, 0.9),row.names(ind_coord),'')),max.overlaps = 150)
susceptible_vector_cell=ind_coord[(ind_coord$pc_combined < quantile(ind_coord$pc_combined, 0.1)),]
Resistant_vector_cell=ind_coord[(ind_coord$pc_combined > quantile(ind_coord$pc_combined, 0.9)),]


HIV1_vector_data_notable <- as.data.frame(rbind(HIV_vector_data_resistant, HIV_vector_data_susceptible)) %>%
  mutate(phenotype = ifelse(row.names(.) %in% row.names(HIV_vector_data_resistant),
                            "resistant",
                            "susceptible"))
```

here i create the classification based on the clusters, i set the clusters and choose the most extreme groups, NOTE- k means code changes every time so i set the seed. if the seed is changed this code would have to be changed also

```{r}
susceptible_vector_clusters=ind_coord[(ind_coord$group==8),]#|(ind_coord$group==5),]
Resistant_vector_clusters=ind_coord[(ind_coord$group==6)|(ind_coord$group==4),]
```

## visualising data split

here i visualise the extremes i found, and see the split between them in a graph, can see how well the separation is by splitting between the parameters, can also see how well parameters align to biological expectations

here i look at assay output(zscore) there is a solid split between the susceptible and resistant with suscpetibles consistenly having higher output (which is the max output seen)

```{r}
ggplot(HIV1_vector_data_notable,aes(x=row.names(HIV1_vector_data_notable),y=assay_output,colour = phenotype))+
  geom_point()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

this looks at the a log (the y offset in our logarithmic function), there is a clear difference between the values with the resistant having lower y offsets which is what you would expect though some are near 0 but all the susceptible are above zero meaning showing they would be more permissible

```{r}
ggplot(HIV1_vector_data_notable,aes(x=row.names(HIV1_vector_data_notable),y=a_log,colour = phenotype))+
  geom_point()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

b_log is the slope of the curve on the logarithmic function, Theoretically this would mean the more permissible/susceptible cell line the higher the b_log value.However this is not observed instead the opposite is observed where the resistant have higher b_log. This is potentially because the b_log value is compensating for the more extreme a_log values to compensate

```{r}
ggplot(HIV1_vector_data_notable,aes(x=row.names(HIV1_vector_data_notable),y=b_log,colour = phenotype))+
  geom_point()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

This also has a clear split which biologically makes sense, though biologically having a high absolute AUC doesn't neccesarily mean that it is a extremely susceptible, but it does imply something about the dynamics of the and how it responds vector

```{r}
ggplot(HIV1_vector_data_notable,aes(x=row.names(HIV1_vector_data_notable),y=area_under_curve,colour = phenotype))+
  geom_point()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

```{r}
ggplot(HIV1_vector_data_notable,aes(x=row.names(HIV1_vector_data_notable),y=logis_b,colour = phenotype))+
  geom_point()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

logis_d is the max from the 4 parameter logistic, it mostly would align with the assay output, but the logistic may find a higher max depending on the shape of the fitted curve, e.g if the curve was increasing but hadn't reached a plateau the model would model a theorised plateau that may be higher than assay output, so in this way it measures both the max output and the dynamics of the gfp production

we see a good split except for one cell line Febc which is resistant but has a high value. This could mean the curve of that cell followed a more gradual increase but still was modelled a high theorised plateau.

```{r}
ggplot(HIV1_vector_data_notable,aes(x=row.names(HIV1_vector_data_notable),y=logis_d,colour = phenotype))+
  geom_point()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

Logis e works as the x offset, so having a high logis e shifts the graph to the right which would theoretically make it more resistant . the graph is not as split, with a lot of overlap between susceptible and resistant but many of the resistant have higher logis e than susceptible which would be expected.

```{r}
#ggplot(HIV1_vector_data_notable,aes(x=row.names(HIV1_vector_data_notable),y=logis_e,colour = phenotype))+
  #geom_point()+
  #theme(axis.title.x=element_blank(),
        #axis.text.x=element_blank(),
        #axis.ticks.x=element_blank())
```

c log is the x offset in the logarithmic model, this means that theoretically the susceptible would have lower c log but this is not what is seen, instead we see the opposite that only the resistant have low c log, However c_log is highly correlated with a_log which does fit what would be expected. It could be that due to a quirk in the modelling, the shape of the highly susceptible affected the way c log was modeled to have a correct fit, compared to the more flat growth of the resistant curves which may have been modeled to have much lower c_log.

```{r}
#ggplot(HIV1_vector_data_notable,aes(x=row.names(HIV1_vector_data_notable),y=logis_c,colour = phenotype))+
  #geom_point()+
  #theme(axis.title.x=element_blank(),
        #axis.text.x=element_blank(),
        #axis.ticks.x=element_blank())
```

## ranking list

Here i rank and gain the top cell lines in each parameter to see which cell lines appear the most,this is another way of separating the data but i don't prefer it

```{r}
vector_ranking=list()
for(col in names(HIV1_vector_data_PCA_1)) {
  
  # Extract the column values
  vec <- HIV1_vector_data_PCA_1[[col]]
  
  # Calculate the 10th and 90th quantiles (ignoring NAs)
  low_quant  <- quantile(vec, 0.1, na.rm = TRUE)
  high_quant <- quantile(vec, 0.9, na.rm = TRUE)
  
  # Identify rows where the value is either below the 10th percentile or above the 90th percentile
  extreme_idx <- which(vec < low_quant | vec > high_quant)
  
  # Subset the original data frame for these extreme values
  subset_data <- HIV1_vector_data_PCA_1[extreme_idx, ]
  
  # Add a phenotype column: if the value is below the 10th percentile, call it "resistant",
  # if above the 90th percentile, label it "susceptible"
  # (This uses the current column's values from the subset.)
  subset_data$phenotype <- ifelse(vec[extreme_idx] < low_quant, "resistant", "susceptible")
  subset_data$cell_line=rownames(subset_data)
  rownames(subset_data) <- NULL
  
  # Store the subset in the vector_ranking list using the column name as the key
  vector_ranking[[col]] <- subset_data
}

#for(col in names(HIV1_vector_data_PCA_1[,c(4,5,8)])) {
  
  # Extract the column values
  #vec <- HIV1_vector_data_PCA_1[[col]]
  
  # Calculate the 10th and 90th quantiles (ignoring NAs)
  #low_quant  <- quantile(vec, 0.1, na.rm = TRUE)
  #high_quant <- quantile(vec, 0.9, na.rm = TRUE)
  
  # Identify rows where the value is either below the 10th percentile or above the 90th percentile
  #extreme_idx <- which(vec < low_quant | vec > high_quant)
  
  # Subset the original data frame for these extreme values
  #subset_data <- HIV1_vector_data_PCA_1[extreme_idx, ]
  
  # Add a phenotype column: if the value is below the 10th percentile, call it "resistant",
  # if above the 90th percentile, label it "susceptible"
  # (This uses the current column's values from the subset.)
  #subset_data$phenotype <- ifelse(vec[extreme_idx] < low_quant, "susceptible","resistant")
  #subset_data$cell_line=rownames(subset_data)
  #rownames(subset_data) <- NULL
  
  # Store the subset in the vector_ranking list using the column name as the key
  #vector_ranking[[col]] <- subset_data
#}


```

```{r}
vector_ranking_df=bind_rows(vector_ranking)
cell_line_counts <- vector_ranking_df %>% 
  count(cell_line,phenotype)

# 3. Get the cell lines that appear more than 9 times.
common_vector_cell_lines <- cell_line_counts[cell_line_counts$n>2,]

# 4. Filter the original data to include only those common_vector cell lines.
most_common_vector <- inner_join(vector_ranking_df,common_vector_cell_lines,join_by(x$cell_line==y$cell_line,x$phenotype==y$phenotype))

# Optionally, view the result:
unique(most_common_vector)

common_vector_list=unique(most_common_vector)

```


```{r}

export_list_for_plink=ind_coord[,c(1,2)]
export_list_for_plink$cell_line=row.names(export_list_for_plink)
export_list_for_plink=export_list_for_plink[,c(3,1)]
write_delim(x = export_list_for_plink,file = "plink_vector_phenotype.txt",delim = "\t")

```



things for ask luis

-   logistic regression problems (especially in the virus)
-   clustering vs using quartiles vs ranking for classification
    -   number of clusters to use
-   alignment with Neo's data
    -   some cell lines missing in Neo's data
-   How many parameters is needed
-   do i have to have the exact same parameters for both
-   Weird data in virus sets
-   With the VCF (ask him if he can help me with it )
-   Mention
    -   outlier function
    -   absolute AUC ( due to using negatives due to zscore)
    -   
